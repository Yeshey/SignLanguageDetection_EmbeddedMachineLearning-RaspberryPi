{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SignLanguageDetection_EmbeddedMachineLearning-RaspberryPi\n",
    "\n",
    "Project to deploy a Machine Learning Model on a Rasberry Pi.  \n",
    "Training a model to identify american sign language letters in images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Time setup  \n",
    "- You're expected to have [install](https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/install.html#tf-install)ed [tensorflow in your PC](https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/install.html#tf-install), check the References in the bottom for notes.\n",
    "- After that, in your conda environment, there are a several dependencies you must install:\n",
    "  - if you created your connda environment with `conda create -n signLanguageDetector pip python=3.9` for example.\n",
    "  - run `conda activate signLanguageDetector`\n",
    "  - `pip install wget hazm pandas` and other libraries that appear that might be needed.\n",
    "- To download the dataset and the pre-trained-model run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget \n",
    "import zipfile\n",
    "import os\n",
    "import tarfile\n",
    "\n",
    "print(\"Running first time setup...\")\n",
    "\n",
    "# check folders\n",
    "ckckfldrs = os.listdir(\"./TensorFlow/workspace/training_demo/\")\n",
    "\n",
    "if all( i != \"images\" for i in ckckfldrs):\n",
    "    os.mkdir(\"./TensorFlow/workspace/training_demo/images\")\n",
    "    # Downloading database\n",
    "    print(\"thanks to https://public.roboflow.com/object-detection/american-sign-language-letters/1 for dataset\")\n",
    "    print(\"Downloading Dataset to ./TensorFlow/workspace/training_demo/images/...\")\n",
    "    url = 'https://public.roboflow.com/ds/wdx82NVcss?key=lyVASY8xq4'\n",
    "    path = './TensorFlow/workspace/training_demo/images/'\n",
    "    wget.download(url,out = path)\n",
    "\n",
    "    print(\"\\nUnzipping...\")\n",
    "    zipPath = os.listdir(\"./TensorFlow/workspace/training_demo/images/\")\n",
    "    zipPath = \"./TensorFlow/workspace/training_demo/images/\" + str(zipPath[0])\n",
    "    print(zipPath)\n",
    "    with zipfile.ZipFile(zipPath, 'r') as zip_ref:\n",
    "        zip_ref.extractall(\"./TensorFlow/workspace/training_demo/images/\")\n",
    "\n",
    "    for item in os.listdir(\"./TensorFlow/workspace/training_demo/images/\"): # loop through items in dir\n",
    "        if item.endswith(\".zip\"): # check for \".zip\" extension\n",
    "            os.remove(\"./TensorFlow/workspace/training_demo/images/\"+ item)\n",
    "else:\n",
    "    print(\"images folder already exists\")\n",
    "\n",
    "if all( i != \"pre-trained-models\" for i in ckckfldrs):\n",
    "    os.mkdir(\"./TensorFlow/workspace/training_demo/pre-trained-models\")\n",
    "    # Downloading Pre-trained model\n",
    "    print(\"using SSD ResNet50 V1 FPN 640x640 pre-trained model from https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md\")\n",
    "    print(\"Downloading pre-trained model to ./TensorFlow/workspace/training_demo/pre-trained-models/..\")\n",
    "    url = 'http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz'\n",
    "    path = './TensorFlow/workspace/training_demo/pre-trained-models/'\n",
    "    wget.download(url,out = path)\n",
    "\n",
    "    print(\"\\nUnzipping...\")\n",
    "    zipPath = os.listdir(\"./TensorFlow/workspace/training_demo/pre-trained-models/\")\n",
    "    zipPath = \"./TensorFlow/workspace/training_demo/pre-trained-models/\" + str(zipPath[0])\n",
    "    tar = tarfile.open(zipPath, \"r:gz\")\n",
    "    tar.extractall('./TensorFlow/workspace/training_demo/pre-trained-models/')\n",
    "    tar.close()\n",
    "\n",
    "    for item in os.listdir(\"./TensorFlow/workspace/training_demo/pre-trained-models/\"): # loop through items in dir\n",
    "        if item.endswith(\".tar.gz\"): # check for \".zip\" extension\n",
    "            os.remove(\"./TensorFlow/workspace/training_demo/pre-trained-models/\"+ item)\n",
    "else:\n",
    "    print(\"pre-trained-models folder already exists\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Generate .record files from labeled dataset](https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html#create-tensorflow-records)\n",
    "(if you're using the same dataset as me, they're already generated!)  \n",
    "Something like:\n",
    "\n",
    "```bash\n",
    "# Create train data:\n",
    "python ./TensorFlow/scripts/preprocessing/generate_tfrecord.py -x [PATH_TO_IMAGES_FOLDER]/train -l [PATH_TO_ANNOTATIONS_FOLDER]/label_map.pbtxt -o [PATH_TO_ANNOTATIONS_FOLDER]/train.record\n",
    "\n",
    "# Create test data:\n",
    "python ./TensorFlow/scripts/preprocessing/generate_tfrecord.py -x [PATH_TO_IMAGES_FOLDER]/test -l [PATH_TO_ANNOTATIONS_FOLDER]/label_map.pbtxt -o [PATH_TO_ANNOTATIONS_FOLDER]/test.record\n",
    "\n",
    "# For example\n",
    "# python generate_tfrecord.py -x C:/Users/sglvladi/Documents/Tensorflow/workspace/training_demo/images/train -l C:/Users/sglvladi/Documents/Tensorflow/workspace/training_demo/annotations/label_map.pbtxt -o C:/Users/sglvladi/Documents/Tensorflow/workspace/training_demo/annotations/train.record\n",
    "# python generate_tfrecord.py -x C:/Users/sglvladi/Documents/Tensorflow/workspace/training_demo/images/test -l C:/Users/sglvladi/Documents/Tensorflow2/workspace/training_demo/annotations/label_map.pbtxt -o C:/Users/sglvladi/Documents/Tensorflow/workspace/training_demo/annotations/test.record\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if all( i != \"model_main_tf2.py\" for i in os.listdir(\".\")):\n",
    "    os.chdir('./TensorFlow/workspace/training_demo/') # Change the working directory\n",
    "    print(f\"working directory now: {os.getcwd()}\")\n",
    "%run -i 'model_main_tf2.py' --model_dir=models/my_ssd_resnet50_v1_fpn --pipeline_config_path=models/my_ssd_resnet50_v1_fpn/pipeline.config\n",
    "os.chdir('../../../')\n",
    "print(f\"working directory now: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Monitor Training Job Progress using TensorBoard](https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html#tensorboard-sec)\n",
    "1. While the model trains, you can \n",
    "   1. cd into the training_demo folder\n",
    "   2. run `tensorboard --logdir=models/my_ssd_resnet50_v1_fpn` \n",
    "   3. access http://localhost:6006/ in your browser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Evaluating the Model](https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html#evaluating-the-model-optional)\n",
    "1. After the model has been trained, you can see how good it does by:\n",
    "   1. cd into the training_demo folder\n",
    "   2. run `python model_main_tf2.py --model_dir=models/my_ssd_resnet50_v1_fpn --pipeline_config_path=models/my_ssd_resnet50_v1_fpn/pipeline.config --checkpoint_dir=models/my_ssd_resnet50_v1_fpn`\n",
    "   3. access http://localhost:6006/ in your browser\n",
    "   4. seing in the images section, the eval:side_by_side images to see how good it is detecting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Exporting the Trained Model](https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html#exporting-a-trained-model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if all( i != \"exporter_main_v2.py\" for i in os.listdir(\".\")):\n",
    "    os.chdir('./TensorFlow/workspace/training_demo/') # Change the working directory\n",
    "    print(f\"working directory now: {os.getcwd()}\")\n",
    "%run -i './exporter_main_v2.py' --input_type image_tensor --pipeline_config_path ./models/my_ssd_resnet50_v1_fpn/pipeline.config --trained_checkpoint_dir ./models/my_ssd_resnet50_v1_fpn/ --output_directory ./exported-models/my_model\n",
    "os.chdir('../../../')\n",
    "print(f\"working directory now: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Detect Objects Using Your Webcam](https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/auto_examples/object_detection_camera.html#detect-objects-using-your-webcam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.join(os.getcwd(), 'TensorFlow/workspace/training_demo/')\n",
    "MODELS_DIR = os.path.join(DATA_DIR, 'exported-models')\n",
    "MODEL_NAME = 'my_model_american_alphabet_sign_language'\n",
    "PATH_TO_CKPT = os.path.join(MODELS_DIR, os.path.join(MODEL_NAME, 'checkpoint/'))\n",
    "PATH_TO_CFG = os.path.join(MODELS_DIR, os.path.join(MODEL_NAME, 'pipeline.config'))\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'    # Suppress TensorFlow logging\n",
    "import tensorflow as tf\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "from object_detection.builders import model_builder\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')           # Suppress TensorFlow logging (2)\n",
    "\n",
    "# Enable GPU dynamic memory allocation\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "# Load pipeline config and build a detection model\n",
    "configs = config_util.get_configs_from_pipeline_file(PATH_TO_CFG)\n",
    "model_config = configs['model']\n",
    "detection_model = model_builder.build(model_config=model_config, is_training=False)\n",
    "\n",
    "# Restore checkpoint\n",
    "ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
    "ckpt.restore(os.path.join(PATH_TO_CKPT, 'ckpt-0')).expect_partial()\n",
    "\n",
    "@tf.function\n",
    "def detect_fn(image):\n",
    "    \"\"\"Detect objects in image.\"\"\"\n",
    "\n",
    "    image, shapes = detection_model.preprocess(image)\n",
    "    prediction_dict = detection_model.predict(image, shapes)\n",
    "    detections = detection_model.postprocess(prediction_dict, shapes)\n",
    "\n",
    "    return detections, prediction_dict, tf.reshape(shapes, [-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load label map data (for plotting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_LABELS = os.path.join(os.getcwd(), 'TensorFlow/workspace/training_demo/annotations/label_map.pbtxt')\n",
    "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS,\n",
    "                                                                    use_display_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the video stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@0.021] global /io/opencv/modules/videoio/src/cap_v4l.cpp (889) open VIDEOIO(V4L2:/dev/video0): can't open camera by index\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "StagingError",
     "evalue": "in user code:\n\n    File \"/tmp/ipykernel_90630/1209127725.py\", line 34, in detect_fn  *\n        image, shapes = detection_model.preprocess(image)\n    File \"/home/yeshey/anaconda3/envs/signLanguageDetector/lib/python3.9/site-packages/object_detection/meta_architectures/ssd_meta_arch.py\", line 482, in preprocess  *\n        normalized_inputs = self._feature_extractor.preprocess(inputs)\n    File \"/home/yeshey/anaconda3/envs/signLanguageDetector/lib/python3.9/site-packages/object_detection/models/ssd_resnet_v1_fpn_keras_feature_extractor.py\", line 203, in preprocess  *\n        if resized_inputs.shape.as_list()[3] == 3:\n\n    IndexError: list index out of range\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStagingError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/mnt/DataDisk/PersonalFiles/2022/HAW/EmbeddedMachineLearning/SignLanguageDetection_EmbeddedMachineLearning-RaspberryPi/SignLanguageDetection.ipynb Cell 19'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/mnt/DataDisk/PersonalFiles/2022/HAW/EmbeddedMachineLearning/SignLanguageDetection_EmbeddedMachineLearning-RaspberryPi/SignLanguageDetection.ipynb#ch0000018?line=9'>10</a>\u001b[0m \u001b[39m# Things to try:\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/mnt/DataDisk/PersonalFiles/2022/HAW/EmbeddedMachineLearning/SignLanguageDetection_EmbeddedMachineLearning-RaspberryPi/SignLanguageDetection.ipynb#ch0000018?line=10'>11</a>\u001b[0m \u001b[39m# Flip horizontally\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/mnt/DataDisk/PersonalFiles/2022/HAW/EmbeddedMachineLearning/SignLanguageDetection_EmbeddedMachineLearning-RaspberryPi/SignLanguageDetection.ipynb#ch0000018?line=11'>12</a>\u001b[0m \u001b[39m# image_np = np.fliplr(image_np).copy()\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/mnt/DataDisk/PersonalFiles/2022/HAW/EmbeddedMachineLearning/SignLanguageDetection_EmbeddedMachineLearning-RaspberryPi/SignLanguageDetection.ipynb#ch0000018?line=14'>15</a>\u001b[0m \u001b[39m# image_np = np.tile(\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/mnt/DataDisk/PersonalFiles/2022/HAW/EmbeddedMachineLearning/SignLanguageDetection_EmbeddedMachineLearning-RaspberryPi/SignLanguageDetection.ipynb#ch0000018?line=15'>16</a>\u001b[0m \u001b[39m#     np.mean(image_np, 2, keepdims=True), (1, 1, 3)).astype(np.uint8)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/mnt/DataDisk/PersonalFiles/2022/HAW/EmbeddedMachineLearning/SignLanguageDetection_EmbeddedMachineLearning-RaspberryPi/SignLanguageDetection.ipynb#ch0000018?line=17'>18</a>\u001b[0m input_tensor \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconvert_to_tensor(np\u001b[39m.\u001b[39mexpand_dims(image_np, \u001b[39m0\u001b[39m), dtype\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mfloat32)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/mnt/DataDisk/PersonalFiles/2022/HAW/EmbeddedMachineLearning/SignLanguageDetection_EmbeddedMachineLearning-RaspberryPi/SignLanguageDetection.ipynb#ch0000018?line=18'>19</a>\u001b[0m detections, predictions_dict, shapes \u001b[39m=\u001b[39m detect_fn(input_tensor)\n\u001b[1;32m     <a href='vscode-notebook-cell:/mnt/DataDisk/PersonalFiles/2022/HAW/EmbeddedMachineLearning/SignLanguageDetection_EmbeddedMachineLearning-RaspberryPi/SignLanguageDetection.ipynb#ch0000018?line=20'>21</a>\u001b[0m label_id_offset \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/mnt/DataDisk/PersonalFiles/2022/HAW/EmbeddedMachineLearning/SignLanguageDetection_EmbeddedMachineLearning-RaspberryPi/SignLanguageDetection.ipynb#ch0000018?line=21'>22</a>\u001b[0m image_np_with_detections \u001b[39m=\u001b[39m image_np\u001b[39m.\u001b[39mcopy()\n",
      "File \u001b[0;32m~/anaconda3/envs/signLanguageDetector/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/yeshey/anaconda3/envs/signLanguageDetector/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py?line=150'>151</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    <a href='file:///home/yeshey/anaconda3/envs/signLanguageDetector/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py?line=151'>152</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m--> <a href='file:///home/yeshey/anaconda3/envs/signLanguageDetector/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py?line=152'>153</a>\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    <a href='file:///home/yeshey/anaconda3/envs/signLanguageDetector/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py?line=153'>154</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/yeshey/anaconda3/envs/signLanguageDetector/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py?line=154'>155</a>\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/signLanguageDetector/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py:1127\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/yeshey/anaconda3/envs/signLanguageDetector/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py?line=1124'>1125</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/yeshey/anaconda3/envs/signLanguageDetector/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py?line=1125'>1126</a>\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> <a href='file:///home/yeshey/anaconda3/envs/signLanguageDetector/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py?line=1126'>1127</a>\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mag_error_metadata\u001b[39m.\u001b[39mto_exception(e)\n\u001b[1;32m   <a href='file:///home/yeshey/anaconda3/envs/signLanguageDetector/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py?line=1127'>1128</a>\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   <a href='file:///home/yeshey/anaconda3/envs/signLanguageDetector/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py?line=1128'>1129</a>\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mStagingError\u001b[0m: in user code:\n\n    File \"/tmp/ipykernel_90630/1209127725.py\", line 34, in detect_fn  *\n        image, shapes = detection_model.preprocess(image)\n    File \"/home/yeshey/anaconda3/envs/signLanguageDetector/lib/python3.9/site-packages/object_detection/meta_architectures/ssd_meta_arch.py\", line 482, in preprocess  *\n        normalized_inputs = self._feature_extractor.preprocess(inputs)\n    File \"/home/yeshey/anaconda3/envs/signLanguageDetector/lib/python3.9/site-packages/object_detection/models/ssd_resnet_v1_fpn_keras_feature_extractor.py\", line 203, in preprocess  *\n        if resized_inputs.shape.as_list()[3] == 3:\n\n    IndexError: list index out of range\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "while True:\n",
    "    # Read frame from camera\n",
    "    ret, image_np = cap.read()\n",
    "\n",
    "    # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
    "    image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "\n",
    "    # Things to try:\n",
    "    # Flip horizontally\n",
    "    # image_np = np.fliplr(image_np).copy()\n",
    "\n",
    "    # Convert image to grayscale\n",
    "    # image_np = np.tile(\n",
    "    #     np.mean(image_np, 2, keepdims=True), (1, 1, 3)).astype(np.uint8)\n",
    "\n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "    detections, predictions_dict, shapes = detect_fn(input_tensor)\n",
    "\n",
    "    label_id_offset = 1\n",
    "    image_np_with_detections = image_np.copy()\n",
    "\n",
    "    viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "          image_np_with_detections,\n",
    "          detections['detection_boxes'][0].numpy(),\n",
    "          (detections['detection_classes'][0].numpy() + label_id_offset).astype(int),\n",
    "          detections['detection_scores'][0].numpy(),\n",
    "          category_index,\n",
    "          use_normalized_coordinates=True,\n",
    "          max_boxes_to_draw=200,\n",
    "          min_score_thresh=.30,\n",
    "          agnostic_mode=False)\n",
    "\n",
    "    # Display output\n",
    "    cv2.imshow('object detection', cv2.resize(image_np_with_detections, (800, 600)))\n",
    "\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Convert a model to Lite model](https://www.tensorflow.org/lite/convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "saved_model_dir = \"./TensorFlow/workspace/training_demo/exported-models/my_model_american_alphabet_sign_language/saved_model\"\n",
    "\n",
    "# Convert the model\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir) # path to the SavedModel directory\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "with open('model.tflite', 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "### Training the model\n",
    "1. Following [*Training Custom Object Detector*](https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html) tutorial\n",
    "     - [TensorFlow Installation](https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/install.html#tf-install) Notes:\n",
    "       - With Anaconda installed, use `conda info --envs` to see created environments\n",
    "       - Before working on the project, do `conda activate signLanguageDetector` to activate the environment every time\n",
    "       - Install cuda and cudnn tu use GPU on manjaro with `sudo pacman -S cuda cudnn`\n",
    "       - For Object detection API, added the [TensorFlow Models repository](https://github.com/tensorflow/models) as a git submodule with the command `git submodule add https://github.com/tensorflow/models ./TensorFlow/models/`\n",
    "       - To test if everything worked out in the installation you can run `python TensorFlow/models/research/object_detection/builders/model_builder_tf2_test.py`\n",
    "      - Dataset from roboflow.com: [American Sign Language Letters Dataset](https://public.roboflow.com/object-detection/american-sign-language-letters/1)\n",
    "      - Had to do [this](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2.md#python-package-installation) before training to fix an error\n",
    "2. [Detect Objects Using Your Webcam](https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/auto_examples/object_detection_camera.html#detect-objects-using-your-webcam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "832373f34c5392a1ac8810e042c819613272440a316383854975927ea4f31b34"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('signLanguageDetector')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
